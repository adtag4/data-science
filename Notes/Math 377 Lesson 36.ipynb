{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 36: Multiple Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lesson we will be exploring multiple regression. This is a regression model with more than one predictor. Again, there are two, non-mutually exclusive, ideas about regression. The first is to use regression to understand the relationship between the predictors, also called features, inputs, or independent variables and the response, also called output, or dependent variable. For our model we will have only one response. In this capacity, we often are interested in inference. The second use of regression models is for prediction. Here we are only concerned with the predictive performance of the model and not the functional relationship between the inputs and the output.  This is how linear regression is considered a part of the machine learning ecology, as a supervised, model-based, batch learning method.   \n",
    "\n",
    "Let's start by importing our packages and performing exploritory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has been inspired from [Datacamp's](www.datacamp.com) Supervised Learning with scikit-learn course and the book Hands-On Machine Learning with Scikit-Learn & Tensorflow by Aurelien Geron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "First we will import the usual packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import *\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will import some new packages that you will start to learn. These include [scikit-learn](https://scikit-learn.org/stable/), [statsmodel](https://www.statsmodels.org/stable/index.html), and [seaborn](https://seaborn.pydata.org/). These should have been installed as part of the Anaconda distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model, datasets\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation  \n",
    "\n",
    "As future military officers and leaders, we need to appreciate our understanding and view of the world. Take the following test, http://forms.gapminder.org/s3/test-2018, without looking up answers, do it in earnest. Report your score in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4/13, or 31%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will be working with a subset of Gapminder data, see the [Gapminder](https://www.gapminder.org/) website for more information on this wonderful project by Hans Rosling and his team.  A subset of the data is provided to you in one CSV file called `gm_2008_region.csv`. Specifically, your goal will be to use this data to predict the life expectancy in a given country based on features such as the country's GDP, fertility rate, and population. This dataset has been heavily preprocessed to save you work.\n",
    "\n",
    "Since the target variable, life expectancy, is quantitative, this is a regression problem. To begin, you will fit a linear regression with just one feature: `fertility`, which is the average number of children a woman in a given country gives birth to. In later exercises, you will use all the features to build regression models.\n",
    "\n",
    "Before that, however, you need to import the data and get it into the form needed by `scikit-learn`. Then we will use `statsmodels` to get more detailed output. This involves creating feature and target variable arrays. Furthermore, since you are going to use only one feature to begin with, you need to do some reshaping using NumPy's `.reshape()` method. Don't worry too much about this reshaping right now, but it is something you will have to do occasionally when working with `scikit-learn` so it is useful to practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of y before reshaping: (139,)\n",
      "Dimensions of X before reshaping: (139,)\n",
      "Dimensions of y after reshaping: (139, 1)\n",
      "Dimensions of X after reshaping: (139, 1)\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame: df\n",
    "df = pd.read_csv('gm_2008_region.csv')\n",
    "\n",
    "# Create arrays for features and target variable\n",
    "y = df['life']\n",
    "X = df['fertility']\n",
    "\n",
    "# Print the dimensions of X and y before reshaping\n",
    "print(\"Dimensions of y before reshaping: {}\".format(y.shape))\n",
    "print(\"Dimensions of X before reshaping: {}\".format(X.shape))\n",
    "\n",
    "# Reshape X and y\n",
    "y = y.values.reshape(-1,1)\n",
    "X = X.values.reshape(-1,1)\n",
    "\n",
    "# Print the dimensions of X and y after reshaping\n",
    "print(\"Dimensions of y after reshaping: {}\".format(y.shape))\n",
    "print(\"Dimensions of X after reshaping: {}\".format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the Gapminder data\n",
    "\n",
    "Start exploring the data using `pandas` methods such as `.info()`, `.describe()`, `.corr()`, and `.head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's look at some visual summaries of the data. The `pandas` package allows us to easily invoke the `matplotlib` to explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(bins=11,figsize=(20,15));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax =sns.countplot(x='Region', data=df, palette='RdBu',orient=\"h\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxplot of life expectancy per region\n",
    "df.boxplot('life','Region', rot=60);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(df[df.columns[:8]],figsize=(15,15));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will construct a heatmap showing the correlation between the different features of the Gapminder dataset. Cells that are in green show positive correlation, while cells that are in red show negative correlation. Take a moment to explore this: Which features are positively correlated with life, and which ones are negatively correlated? Does this match your intuition?\n",
    "\n",
    "The heatmap can be generated using Seaborn's heatmap function and the following line of code, where `df.corr()` computes the pairwise correlation between columns:\n",
    "\n",
    "`sns.heatmap(df.corr(), square=True, cmap='RdYlGn')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(), square=True, cmap='RdYlGn');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and Predict a Single Variable\n",
    "\n",
    "Now, you will fit a linear regression and predict life expectancy using just one feature. In this exercise, you will use the `fertility` feature of the Gapminder dataset. Since the goal is to predict life expectancy, the target variable here is `life`. \n",
    "\n",
    "First, create a scatter plot with `fertility` on the x-axis and `life` on the y-axis. As you will see, there is a strongly negative correlation, so a linear regression should be able to capture this trend. Your job is to fit a linear regression and then predict the life expectancy, overlaying these predicted values on the plot to generate a regression line. You will also compute and print the $R^2$ score using `sckit-learn's .score()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename X so it makes more sense to us\n",
    "X_fertility = X\n",
    "\n",
    "plt.title('Linear Regression')\n",
    "plt.scatter(X_fertility, y)\n",
    "plt.xlabel('Fertility')\n",
    "plt.ylabel('Life Expectancy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit the model we need to create a `LinearRegression` instance from the `scikit-learn` package. This package has a consistent API, where from an instance we either create an estimator, a transform, or a predictor. To create an estimator, use the `.fit` method. See the [scikit-learn](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares) help page for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the regressor: reg\n",
    "reg = linear_model.LinearRegression()\n",
    "\n",
    "# Create the prediction space\n",
    "prediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1,1)\n",
    "\n",
    "# Fit the model to the data\n",
    "reg.fit(X_fertility,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will predict the response for the `prediction_space` we created above and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions over the prediction space: y_pred\n",
    "y_pred = reg.predict(prediction_space)\n",
    "\n",
    "# Plot regression line\n",
    "plt.title('Linear Regression')\n",
    "plt.scatter(X_fertility, y)\n",
    "plt.xlabel('Fertility')\n",
    "plt.ylabel('Life Expectancy')\n",
    "plt.plot(prediction_space, y_pred, color='black', linewidth=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the coefficients and the $R^2$, the percent of variation in life expectancy explained by fertility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Intercept: ',np.round(reg.intercept_[0],4))\n",
    "print('Slope: ',np.round(reg.coef_[0][0],4))\n",
    "# Print R^2 \n",
    "print(\"\\n\")\n",
    "print(\"The coefficeint of determination:\", np.round(reg.score(X_fertility, y),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scikit-learn` package is designed for production model building and is not intuitive for generating models in an interactive manner. The `statsmodels` package is better for this. It also allows us to use the `R` formula notation. In this case the dependent variables are on the left of the tilde and the predictors on the right. Let's model life expectancy as a function of fertility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit regression model (using statsmodels)\n",
    "results = smf.ols('life ~ fertility', data=df).fit()\n",
    "\n",
    "# Inspect the results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output has a great deal of information. The assumptions for this model are that the errors are independently distributed from a normal distribution with mean zero and constant variance. This means the slope coefficient is normally distributed as well, thus we can generate $p$-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $F$-statistic is simultaneously testing if all coefficients are zero:    \n",
    "\n",
    "$$H_{o}: \\beta_1 = \\beta_2 = \\dots = \\beta_n = 0 $$  \n",
    "$$H_{a}: \\mbox{At least one coefficient not 0}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $t$-test is testing a single coefficient.  \n",
    "$$H_{o}: \\beta_i = 0 $$  \n",
    "$$H_{a}: \\beta_i \\neq 0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the normality plot using a quantile-quantile plot. This plot has the quantiles from a normal distribution on one axis and the empirical quantiles on the other. If it is normal, the data will be approximately on a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(results.resid,fit=True,line=\"45\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normality assumption is suspect. We may want to use a bootstrap method instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Regression  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now add more variables to the model. We will first take the approach of inference and then prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model formula\n",
    "\"+\".join(np.delete(df.columns,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_formula = \"life~\" +\"+\".join(np.delete(df.columns,7))\n",
    "my_formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit regression model (using statsmodels)\n",
    "results2 = smf.ols(formula= my_formula , data=df).fit()\n",
    "\n",
    "# Inspect the results\n",
    "print(results2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret the results. \n",
    "\n",
    "1. Why did the region get broken into 5 separate variables?\n",
    "2. Why is $R^2$ and the $F$-statistic so significant, yet many individual $p$-values are not significant?\n",
    "3. What is the predictive performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test subsets of the variables together using an F-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results2.f_test(\"Region[T.East Asia & Pacific] = Region[T.Europe & Central Asia] = Region[T.Middle East & North Africa]=Region[T.South Asia] = Region[T.Sub-Saharan Africa] = 0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results2.f_test(\"population = BMI_male = 0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(results2.resid,fit=True,line=\"45\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model again, but drop region from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.delete(df.columns,[7,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_formula2 = \"life~\" +\"+\".join(np.delete(df.columns,[7,9]))\n",
    "my_formula2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit regression model (using statsmodels)\n",
    "results3 = smf.ols(formula= my_formula2 , data=df).fit()\n",
    "\n",
    "# Inspect the results\n",
    "print(results3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(results3.resid,fit=True,line=\"45\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue adjusting the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_formula3 = \"life~\" +\"+\".join(np.delete(df.columns,[0,1,7,9]))\n",
    "my_formula3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit regression model (using statsmodels)\n",
    "results4 = smf.ols(formula= my_formula3 , data=df).fit()\n",
    "\n",
    "# Inspect the results\n",
    "print(results4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(results4.resid,fit=True,line=\"45\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.regressionplots import plot_leverage_resid2\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "fig = plot_leverage_resid2(results4, ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.regressionplots import influence_plot\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "fig = influence_plot(results4,ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dummy variables\n",
    "`scikit-learn` does not accept non-numerical features. In `statsmodels` 'Region' was automatically taken care for us. We want to include 'Region' as it may contain features very useful in predicting life expectancy. For example, Sub-Saharan Africa has a lower life expectancy compared to Europe and Central Asia. Therefore, if you are trying to predict life expectancy, it would be preferable to retain the 'Region' feature. To do this, you need to binarize it by creating dummy variables, which is what you will do in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables: df_region\n",
    "df_region = pd.get_dummies(df)\n",
    "\n",
    "# Print the columns of df_region\n",
    "print(df_region.columns)\n",
    "\n",
    "# Create dummy variables with drop_first=True: df_region\n",
    "df_region = pd.get_dummies(df,drop_first=True)\n",
    "\n",
    "# Print the new columns of df_region\n",
    "print(df_region.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/test split for regression\n",
    "For a measure of prediction error it is best to use data that was not used in the building of the model. An easy first attempt is to break the data into train and test sets. This is vital to ensure that our supervised learning model is able to generalize well to new data. This is also true for classification models.\n",
    "\n",
    "In this exercise, you will split the Gapminder dataset into training and testing sets, and then fit and predict a linear regression over all features. In addition to computing the $R^2$ score, you will also compute the Root Mean Squared Error (RMSE), which is another commonly used metric to evaluate regression models. The feature array `X` and target variable array `y` have been pre-loaded for you from the DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_region.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_region.drop(columns=[\"life\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_region.drop(columns=[\"life\"])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state=42)\n",
    "\n",
    "# Create the regressor: reg_all\n",
    "reg_all = LinearRegression()\n",
    "\n",
    "# Fit the regressor to the training data\n",
    "reg_all.fit(X_train,y_train)\n",
    "\n",
    "# Predict on the test data: y_pred\n",
    "y_pred = reg_all.predict(X_test)\n",
    "\n",
    "# Compute and print R^2 and RMSE\n",
    "print(\"R^2: {}\".format(reg_all.score(X_test, y_test)))\n",
    "rmse = np.sqrt(mean_squared_error(y_test,y_pred))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
